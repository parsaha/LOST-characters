{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17acf83a-4b5b-4a71-bbf8-edb4d0029f46",
   "metadata": {},
   "source": [
    "NOTE: When referencing episodes of the show, shorthand may be used (e.g., S3E7 is Season 3 Episode 7).\n",
    "\n",
    "# Creating Word Count Tables for Every Episode\n",
    "\n",
    "Since we've gotten our hands dirty in the `Lost Transcripts EDA (Pilot)` notebook, we can start to do the same for every episode in the show.\n",
    "\n",
    "## How to Access Every Episode's Transcript\n",
    "\n",
    "We aren't quite able to scrape every transcript yet since the URLs of the transcripts are differentiated by episode title and not by chronological season/episode pairings. So, we'll need to grab the list of episode names. To do this, I'll scrape the data a [Wikipedia page](https://en.wikipedia.org/wiki/List_of_Lost_episodes) (Lostpedia's list seemed too complicated to scrape and the Pilot EDA notebook showed us how annoying it would be to work with IMDb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520d6d78-2a69-4152-96c7-f36dd842b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "import csv\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9463aafa-9183-471b-80ee-05c329fe07cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-\n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_Lost_episodes'\n",
    "response = get(url)\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7aee5f-a96b-48df-af8b-cb0f38efc3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "containers = html_soup.find_all('td', 'summary')\n",
    "print(type(containers))\n",
    "print(len(containers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b681ea3-da4e-4283-a230-9039b0645bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td class=\"summary\" rowspan=\"1\" style=\"text-align:left\">\"<a href=\"/wiki/Pilot_(Lost)\" title=\"Pilot (Lost)\">Pilot (Part 1)</a>\"</td>\n",
      "<td class=\"summary\" rowspan=\"1\" style=\"text-align:left\">\"<a href=\"/wiki/Pilot_(Lost)\" title=\"Pilot (Lost)\">Pilot (Part 2)</a>\"</td>\n",
      "<td class=\"summary\" rowspan=\"1\" style=\"text-align:left\">\"<a href=\"/wiki/The_End_(Lost)\" title=\"The End (Lost)\">The End</a>\"</td>\n"
     ]
    }
   ],
   "source": [
    "print(containers[0])\n",
    "print(containers[1])\n",
    "print(containers[113])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bade6f-d238-4d7f-af7d-6b1b2417fda3",
   "metadata": {},
   "source": [
    "The scrupulous among you may notice that there are more episodes than there are \"titles\" listed in Wikipedia. You would be correct! The final two episodes of every season are grouped together in Wikipedia because they share a name (and are treated as two parts of the same plot). Normally this would be cause for concern since we would need our transcripts to line up with the titles that we are using to iterate through episodes with. However, we're in luck: such transcripts are grouped together in Lostpedia as well! Therefore, accidentally, everything lines up. (Regarding how to assign ratings to these combined episode pairings, we will simply take the average of the two episodes' ratings.)\n",
    "\n",
    "The only hiccup to look out for seems to be that titles in Wikipedia with parentheses are not named with parentheses in Lostpedia. We will adjust for that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c87b06-5157-49ba-83b9-a18f1b9a13ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pilot (Part 1)',\n",
       " 'Pilot (Part 2)',\n",
       " 'Tabula Rasa',\n",
       " 'Walkabout',\n",
       " 'White Rabbit']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_titles = [title.text[1:-1] for title in containers[:114]]\n",
    "raw_titles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e92d0b88-8673-40ca-801d-d69ac9bcfde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pilot,_Part_1', 'Pilot,_Part_2', 'Tabula_Rasa', 'Walkabout', 'White_Rabbit']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_titles = [title.replace(' (',', ').replace(')','').replace(' ','_').replace('&','%26') if '(' in title else title.replace(' ','_').replace('?','%3F') for title in raw_titles]\n",
    "transcript_titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a1780b-0389-4aa3-8f6f-dda9ccb171f1",
   "metadata": {},
   "source": [
    "At this stage, we should be good to write a for loop that uses the `word_count_matrix.py` file (adapted from the `Lost Transcripts EDA (Pilot)` notebook) to create all of the word count tables we need. \n",
    "\n",
    "## Generating the Tables\n",
    "\n",
    "For troubleshooting reasons, I chose to process each season one at a time; however, it should not be hard to do the entire show in one run. \n",
    "\n",
    "To begin, I ran a single episode as a test (S1E5). I learned some interesting things from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "599ae9c3-f4eb-4a8f-9734-1e02209b8738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White_Rabbit\n"
     ]
    }
   ],
   "source": [
    "from word_count_matrix import word_count_table\n",
    "print(transcript_titles[4])\n",
    "#word_count_table(transcript_titles[4],1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9cefbda-7fb1-40dc-982c-8d2e5a9604bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Character</td>\n",
       "      <td>Word Count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGENT</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BOONE</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHARLIE</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHRISTIAN SHEPHARD</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CLAIRE</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HOTEL MANAGER</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HURLEY</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JACK</td>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JIN</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JINÃ‚</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Jin</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>KATE</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LOCKE</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MARGO</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MEATHEAD</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MEDICAL EXAMINER</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MICHAEL</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SAWYER</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SAYID</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>SHANNON</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>SUN</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>WALT</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>YOUNG JACK</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0           1\n",
       "0            Character  Word Count\n",
       "1                AGENT          27\n",
       "2                BOONE         176\n",
       "3              CHARLIE         274\n",
       "4   CHRISTIAN SHEPHARD         159\n",
       "5               CLAIRE         178\n",
       "6        HOTEL MANAGER         100\n",
       "7               HURLEY          90\n",
       "8                 JACK         731\n",
       "9                  JIN          15\n",
       "10               JINÃ‚Â           15\n",
       "11                 Jin          12\n",
       "12                KATE         272\n",
       "13               LOCKE         293\n",
       "14               MARGO         100\n",
       "15            MEATHEAD          22\n",
       "16    MEDICAL EXAMINER          38\n",
       "17             MICHAEL          29\n",
       "18              SAWYER         165\n",
       "19               SAYID          86\n",
       "20             SHANNON          60\n",
       "21                 SUN          47\n",
       "22                WALT          14\n",
       "23          YOUNG JACK           9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('S1E5_example.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    data = list(reader)\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a9e2d-ce05-4e5f-becb-797a0e78d216",
   "metadata": {},
   "source": [
    "The things that stand out to me are that:\n",
    "* JIN comes up two extra times: once as JINÃ‚ (which simply shows as \"JIN \" in my text editor and in the transcript), and once as Jin. This shows that the data is not as clean as we might have wished it was.\n",
    "* There are characters, like YOUNG JACK, that would ideally be aggregated with their appropriate counterparts (in this case, JACK).\n",
    "    * This becomes harder when considering that some characters \"change\" names throughout the show.\n",
    "      <details> <summary> Spoiler Alert </summary> An example includes Ben Linus whose first introduction has him listed as GALE. </details>\n",
    "\n",
    "The first issue was resolved (retroactively) in the Pilot EDA notebook, while the second issue seems too niche to make a large impact on the overall results. In the case of important discrepancies (like the one listed in the spoiler), I will make edits to the `word_count_matrix.py` file as I discover them. \n",
    "\n",
    "Now that that's out of the way, let's clean up some of the out-of-the-ordinary titles and try all of Season 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f648b9f6-24c9-488c-b0c4-471d63e885f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_titles[23] = 'Exodus,_Part_2'\n",
    "transcript_titles[-16] = 'LA_X,_Parts_1_%26_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfd3e0bc-f0e0-4358-bbfa-2d25c8d573a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_num=1\n",
    "for episode_num in range(24):\n",
    "    word_count_table(transcript_titles[episode_num],season_num,episode_num+1)\n",
    "    time.sleep(random.randint(0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d2f144-ec4a-4c2b-83c9-778416e745ef",
   "metadata": {},
   "source": [
    "It worked! Notice that we put a random amount of pause between requests to avoid having some requests blocked due to bot-like behavior. Let's finish up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88be3dc8-347c-40cb-8057-95f7cca73395",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_num=2\n",
    "for episode_num in range(23):\n",
    "    word_count_table(transcript_titles[24+episode_num],season_num,episode_num+1) \n",
    "    time.sleep(random.randint(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f76c08e7-01b9-467c-b53f-1d3e40143679",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_num=3\n",
    "for episode_num in range(22):\n",
    "    word_count_table(transcript_titles[24+23+episode_num],season_num,episode_num+1)\n",
    "    time.sleep(random.randint(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "419bb3da-5ed9-4a66-b068-173131057544",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_num=4\n",
    "for episode_num in range(13):\n",
    "    word_count_table(transcript_titles[24+23+22+episode_num],season_num,episode_num+1)\n",
    "    time.sleep(random.randint(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2f2f795-cbb0-48f5-b699-89bbb479fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_num=5\n",
    "for episode_num in range(16):\n",
    "    word_count_table(transcript_titles[24+23+22+13+episode_num],season_num,episode_num+1)\n",
    "    time.sleep(random.randint(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd17026c-e910-4e2c-ab4c-5efcac64a04f",
   "metadata": {},
   "source": [
    "At Season 5 I came across a small problem I had to fix in the Python script: sometimes locations (like Tunisia in S5E7) have \"Subtitle:\" show up. An easy fix though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0518fcef-fbbc-4cec-ab20-ecf02c2d4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_num=6\n",
    "word_count_table(transcript_titles[24+23+22+13+16+0],season_num,1)\n",
    "for episode_num in range(1,16):\n",
    "    word_count_table(transcript_titles[24+23+22+13+16+episode_num],season_num,episode_num+2)\n",
    "    time.sleep(random.randint(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eefe52-17b1-4a37-84af-0bc6e5f90b98",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "* https://stackoverflow.com/questions/3411771/best-way-to-replace-multiple-characters-in-a-string\n",
    "* https://stackoverflow.com/questions/10648490/removing-first-appearance-of-word-from-a-string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
